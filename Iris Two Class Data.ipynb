{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "data = pd.read_csv(\"Iris Two Class Data.csv\")\n",
    "\n",
    "print(type(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#Check number of classes\n",
    "classes = data[\"Class\"].nunique()\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    50\n",
      "0    50\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check for imbalanced dataset - number of rows for each class\n",
    "num_classes = data[\"Class\"].value_counts()\n",
    "\n",
    "\n",
    "print(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#Check if there is any missing value in each of the columns.\n",
    "for col in data.columns:\n",
    "    print(data[col].isnull().values.any())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Class  sepal-length  sepal-width  petal-length  petal-width\n",
      "count  100.000000    100.000000   100.000000    100.000000   100.000000\n",
      "mean     0.500000      5.797000     3.201000      3.507000     1.136000\n",
      "std      0.502519      0.945319     0.417906      2.095221     0.918114\n",
      "min      0.000000      4.300000     2.200000      1.000000     0.100000\n",
      "25%      0.000000      5.000000     3.000000      1.500000     0.200000\n",
      "50%      0.500000      5.700000     3.200000      3.200000     1.000000\n",
      "75%      1.000000      6.500000     3.425000      5.525000     2.000000\n",
      "max      1.000000      7.900000     4.400000      6.900000     2.500000\n"
     ]
    }
   ],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55555556 0.31818182 0.77966102 0.70833333]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "Xdata_ = np.array(data.iloc[:,1:])\n",
    "#Data Normalization - ZScore vs MinMax\n",
    "#MinMax\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(Xdata_)\n",
    "new_data = scaler.transform(Xdata_)\n",
    "print(new_data[0])\n",
    "print(type(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.53477633 -0.72388613  1.0039725   0.72686519]\n"
     ]
    }
   ],
   "source": [
    "Xdata = np.array(data.iloc[:,1:])\n",
    "#Data Normalization - ZScore vs MinMax\n",
    "#ZScore\n",
    "scaleer = preprocessing.StandardScaler()\n",
    "scaleer.fit(Xdata)\n",
    "newdata = scaleer.transform(Xdata)\n",
    "print(newdata[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MinMax\n",
    "new_data = pd.DataFrame(data=new_data)\n",
    "\n",
    "mtrain_data = new_data.iloc[:,:]\n",
    "mtrain_data_y = data.iloc[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ZScore\n",
    "newdata = pd.DataFrame(data=newdata)\n",
    "\n",
    "ztrain_data = newdata.iloc[:,:]\n",
    "ztrain_data_y = data.iloc[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mX_train,mX_test,my_train,my_test = train_test_split(mtrain_data,mtrain_data_y,test_size = 0.30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andmerc/Thesis/Project/cv/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(6,))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "mlpmodel = MLPClassifier(hidden_layer_sizes=(6,),activation='relu',solver='adam',alpha=0.0001,max_iter=200)\n",
    "\n",
    "mlpmodel.fit(mX_train,my_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlpmodel.predict(mX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 32]\n",
      " [ 0 37]]\n",
      "0.5428571428571428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,roc_auc_score\n",
    "\n",
    "print(confusion_matrix(my_train,predicted))\n",
    "#print(classification_report(my_train,predicted))\n",
    "print(accuracy_score(my_train,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Confusion Matrix --------------\n",
      "[[ 0 17]\n",
      " [ 0 13]]\n",
      "-----------Classification Report----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        17\n",
      "           1       0.43      1.00      0.60        13\n",
      "\n",
      "    accuracy                           0.43        30\n",
      "   macro avg       0.22      0.50      0.30        30\n",
      "weighted avg       0.19      0.43      0.26        30\n",
      "\n",
      "-----------Accuracy Score-----------------\n",
      "0.43333333333333335\n",
      "----------------AUC-----------------------\n",
      "0.23981900452488691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andmerc/Thesis/Project/cv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predicted_ = mlpmodel.predict(mX_test)\n",
    "preds = mlpmodel.predict_proba(mX_test)\n",
    "preds = preds[:,0]\n",
    "\n",
    "print('-----------Confusion Matrix --------------')\n",
    "print(confusion_matrix(my_test,predicted_))\n",
    "print('-----------Classification Report----------')\n",
    "print(classification_report(my_test,predicted_))\n",
    "print('-----------Accuracy Score-----------------')\n",
    "print(accuracy_score(my_test,predicted_))\n",
    "print('----------------AUC-----------------------')\n",
    "print(roc_auc_score(my_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One\n",
    "#####One\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
